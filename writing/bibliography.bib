@article{blei2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  journal = {Journal of machine Learning research},
  volume = {3},
  number = {Jan},
  pages = {993--1022},
  keywords = {Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\GBD2ZLCH\\Blei et al. - 2003 - Latent dirichlet allocation.pdf}
}

@inproceedings{chang2009,
  title = {Reading Tea Leaves: {{How}} Humans Interpret Topic Models},
  shorttitle = {Reading Tea Leaves},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and {Boyd-Graber}, Jordan L. and Blei, David M.},
  year = {2009},
  pages = {288--296},
  keywords = {NLP,quality\_metrics,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\5FUYJMVD\\Chang et al. - 2009 - Reading tea leaves How humans interpret topic mod.pdf}
}

@Manual{tesseract,
    title = {tesseract: Open Source OCR Engine},
    author = {Jeroen Ooms},
    year = {2022},
    note = {R package version 5.1.0},
    url = {https://CRAN.R-project.org/package=tesseract}
}
@Manual{ldatuning,
    title = {ldatuning: Tuning of the Latent Dirichlet Allocation Models Parameters},
    author = {Murzintcev Nikita},
    year = {2020},
    note = {R package version 1.0.2},
    url = {https://CRAN.R-project.org/package=ldatuning},
  }
  
@Article{tidytext,
    title = {tidytext: Text Mining and Analysis Using Tidy Data Principles in R},
    author = {Julia Silge and David Robinson},
    doi = {10.21105/joss.00037},
    url = {http://dx.doi.org/10.21105/joss.00037},
    year = {2016},
    publisher = {The Open Journal},
    volume = {1},
    number = {3},
    journal = {JOSS},
  }
@inproceedings{bischof2012,
  title = {Summarizing Topical Content with Word Frequency and Exclusivity},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Machine Learning}} ({{ICML-12}})},
  author = {Bischof, Jonathan and Airoldi, Edoardo M.},
  year = {2012},
  pages = {201--208},
  keywords = {Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\HKVJBVDG\\Bischof et Airoldi - 2012 - Summarizing topical content with word frequency an.pdf}
}
@Article{tokenizers,
    title = {Fast, Consistent Tokenization of Natural Language Text},
    author = {Lincoln A. Mullen and Kenneth Benoit and Os Keyes and Dmitry Selivanov and Jeffrey Arnold},
    journal = {Journal of Open Source Software},
    year = {2018},
    volume = {3},
    issue = {23},
    pages = {655},
    url = {https://doi.org/10.21105/joss.00655},
    doi = {10.21105/joss.00655},
  }
@Manual{stopwords,
    title = {stopwords: Multilingual Stopword Lists},
    author = {Kenneth Benoit and David Muhr and Kohei Watanabe},
    year = {2021},
    note = {R package version 2.3},
    url = {https://CRAN.R-project.org/package=stopwords},
  }

@Manual{textstem,
    title = {{textstem}: Tools for stemming and lemmatizing text},
    author = {Tyler W. Rinker},
    address = {Buffalo, New York},
    note = {version 0.1.4},
    year = {2018},
    url = {http://github.com/trinker/textstem},
  }

@article{denny2018,
  title = {Text {{Preprocessing For Unsupervised Learning}}: {{Why It Matters}}, {{When It Misleads}}, {{And What To Do About It}}},
  shorttitle = {Text {{Preprocessing For Unsupervised Learning}}},
  author = {Denny, Matthew J. and Spirling, Arthur},
  year = {2018},
  month = apr,
  journal = {Political Analysis},
  volume = {26},
  number = {2},
  pages = {168--189},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.44},
  urldate = {2020-09-22}
  }
@inproceedings{arun2010,
  title = {On {{Finding}} the {{Natural Number}} of {{Topics}} with {{Latent Dirichlet Allocation}}: {{Some Observations}}},
  shorttitle = {On {{Finding}} the {{Natural Number}} of {{Topics}} with {{Latent Dirichlet Allocation}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Arun, R. and Suresh, V. and Veni Madhavan, C. E. and Narasimha Murthy, M. N.},
  editor = {Zaki, Mohammed J. and Yu, Jeffrey Xu and Ravindran, B. and Pudi, Vikram},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {391--402},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-13657-3_43},
  abstract = {It is important to identify the ``correct'' number of topics in mechanisms like Latent Dirichlet Allocation(LDA) as they determine the quality of features that are presented as features for classifiers like SVM. In this work we propose a measure to identify the correct number of topics and offer empirical evidence in its favor in terms of classification accuracy and the number of topics that are naturally present in the corpus. We show the merit of the measure by applying it on real-world as well as synthetic data sets(both text and images). In proposing this measure, we view LDA as a matrix factorization mechanism, wherein a given corpus C is split into two matrix factors M 1 and M 2 as given by C d*w = M1 d*t x Q t*w . Where d is the number of documents present in the corpus and w is the size of the vocabulary. The quality of the split depends on ``t'', the right number of topics chosen. The measure is computed in terms of symmetric KL-Divergence of salient distributions that are derived from these matrix factors. We observe that the divergence values are higher for non-optimal number of topics \textendash{} this is shown by a 'dip' at the right value for 't'.},
  isbn = {978-3-642-13657-3},
  langid = {english},
  keywords = {NLP,quality\_metrics,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\VT5AJ38F\\Arun et al. - 2010 - On Finding the Natural Number of Topics with Laten.pdf}
}

@article{cao2009,
  title = {A Density-Based Method for Adaptive {{LDA}} Model Selection},
  author = {Cao, Juan and Xia, Tian and Li, Jintao and Zhang, Yongdong and Tang, Sheng},
  year = {2009},
  month = mar,
  journal = {Neurocomputing},
  series = {Advances in {{Machine Learning}} and {{Computational Intelligence}}},
  volume = {72},
  number = {7},
  pages = {1775--1781},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2008.06.011},
  urldate = {2021-07-27},
  abstract = {Topic models have been successfully used in information classification and retrieval. These models can capture word correlations in a collection of textual documents with a low-dimensional set of multinomial distribution, called ``topics''. However, it is important but difficult to select the appropriate number of topics for a specific dataset. In this paper, we study the inherent connection between the best topic structure and the distances among topics in Latent Dirichlet allocation (LDA), and propose a method of adaptively selecting the best LDA model based on density. Experiments show that the proposed method can achieve performance matching the best of LDA without manually tuning the number of topics.},
  langid = {english},
  keywords = {NLP,quality\_metrics,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\MQ3DAN8U\\Cao et al. - 2009 - A density-based method for adaptive LDA model sele.pdf}
}

@article{deveaud2014,
  title = {Accurate and Effective Latent Concept Modeling for Ad Hoc Information Retrieval},
  author = {Deveaud, Romain and SanJuan, Eric and Bellot, Patrice},
  year = {2014},
  month = apr,
  journal = {Document num\'erique},
  volume = {17},
  number = {1},
  pages = {61--84},
  issn = {12795127},
  doi = {10.3166/dn.17.1.61-84},
  urldate = {2021-07-27},
  abstract = {A keyword query is the representation of the information need of a user, and is the result of a complex cognitive process which often results in under-specification. We propose an unsupervised method namely Latent Concept Modeling (LCM) for mining and modeling latent search concepts in order to recreate the conceptual view of the original information need. We use Latent Dirichlet Allocation (LDA) to exhibit highly-specific query-related topics from pseudo-relevant feedback documents. We define these topics as the latent concepts of the user query. We perform a thorough evaluation of our approach over two large ad-hoc TREC collections. Our findings reveal that the proposed method accurately models latent concepts, while being very effective in a query expansion retrieval setting.},
  langid = {english},
  keywords = {NLP,quality\_metrics,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\AF8J45F2\\Deveaud et al. - 2014 - Accurate and effective latent concept modeling for.pdf}
}
@book{grimmer2022,
  title = {{Text As Data: A New Framework for Machine Learning and the Social Sciences}},
  shorttitle = {{Text As Data}},
  author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
  year = {2022},
  month = mar,
  publisher = {{Princeton University Press}},
  isbn = {978-0-691-20755-1},
  langid = {Anglais},
  keywords = {-Lu,NLP,social\_sciences\_methodology,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\Q7ZGW7YD\\Grimmer et al_2022_Text As Data.pdf}
}
@article{roberts2013a,
  title = {The {{Structural Topic Model}} and {{Applied Social Science}}},
  author = {Roberts, Margaret E and Tingley, Dustin and Stewart, Brandon M and Airoldi, Edoardo M},
  year = {2013},
  journal = {Advances in neural information processing systems workshop on topic models: computation, application, and evaluation},
  volume = {4},
  pages = {1--20},
  abstract = {We develop the Structural Topic Model which provides a general way to incorporate corpus structure or document metadata into the standard topic model. Document-level covariates enter the model through a simple generalized linear model framework in the prior distributions controlling either topical prevalence or topical content. We demonstrate the model's use in two applied problems: the analysis of open-ended responses in a survey experiment about immigration policy, and understanding differing media coverage of China's rise.},
  langid = {english},
  keywords = {-Lu,NLP,STM},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\5BXTLUUA\\Roberts et al. - 2013 - The Structural Topic Model and Applied Social Scie.pdf}
}

@article{boyd-graber2017,
  title = {Applications of {{Topic Models}}},
  author = {{Boyd-Graber}, Jordan and Hu, Yuening and Mimno, David},
  year = {2017},
  pages = {158},
  langid = {english},
  keywords = {\_tablet,NLP,survey,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\3NM78T34\\Boyd-Graber et al_2017_Applications of Topic Models.pdf}
}
@article{macanovic2022,
  title = {Text Mining for Social Science \textendash{} {{The}} State and the Future of Computational Text Analysis in Sociology},
  author = {Macanovic, Ana},
  year = {2022},
  month = nov,
  journal = {Social Science Research},
  volume = {108},
  pages = {102784},
  issn = {0049-089X},
  doi = {10.1016/j.ssresearch.2022.102784},
  urldate = {2023-04-30},
  abstract = {The emergence of big data and computational tools has introduced new possibilities for using large-scale textual sources in sociological research. Recent work in sociology of culture, science, and economic sociology has shown how computational text analysis can be used in theory building and testing. This review starts with an introduction of the history of computer-assisted text analysis in sociology and then proceeds to discuss five families of computational methods used in contemporary research. Using exemplary studies, it shows how dictionary methods, semantic and network analysis tools, language models, unsupervised, and supervised machine learning can assist sociologists with different analytical tasks. After presenting recent methodological developments, this review summarizes several important implications of using large datasets and computational methods to infer complex meaning in texts. Finally, it calls researchers from different methodological traditions to adopt text mining tools while remaining mindful of lessons learned from working with conventional data and methods.},
  langid = {english},
  keywords = {\_tablet,machine\_learning,NLP,social\_sciences\_methodology,survey,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\SSJP3JIK\\Macanovic_2022_Text mining for social science â€“ The state and the future of computational text.pdf}
}
@article{nelson2020,
  title = {Computational {{Grounded Theory}}: {{A Methodological Framework}}},
  shorttitle = {Computational {{Grounded Theory}}},
  author = {Nelson, Laura K.},
  year = {2020},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {49},
  number = {1},
  pages = {3--42},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124117729703},
  urldate = {2021-07-30},
  abstract = {This article proposes a three-step methodological framework called computational grounded theory, which combines expert human knowledge and hermeneutic skills with the processing power and pattern recognition of computers, producing a more methodologically rigorous but interpretive approach to content analysis. The first, pattern detection step, involves inductive computational exploration of text, using techniques such as unsupervised machine learning and word scores to help researchers to see novel patterns in their data. The second, pattern refinement step, returns to an interpretive engagement with the data through qualitative deep reading or further exploration of the data. The third, pattern confirmation step, assesses the inductively identified patterns using further computational and natural language processing techniques. The result is an efficient, rigorous, and fully reproducible computational grounded theory. This framework can be applied to any qualitative text as data, including transcribed speeches, interviews, open-ended survey data, or ethnographic field notes, and can address many potential research questions.},
  langid = {english},
  keywords = {-Lu,grounded\_theory,NLP,Quantitative\_Analysis,social\_sciences\_methodology},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\N5ZNPEV7\\Nelson - 2020 - Computational Grounded Theory A Methodological Fr.pdf}
}

@article{griffiths2004,
  title = {Finding Scientific Topics},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  year = {2004},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {101},
  number = {suppl 1},
  pages = {5228--5235},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0307752101},
  urldate = {2021-07-27},
  abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
  chapter = {Colloquium},
  copyright = {Copyright \textcopyright{} 2004, The National Academy of Sciences},
  langid = {english},
  pmid = {14872004},
  keywords = {NLP,quality\_metrics,Topic\_modeling},
  file = {C\:\\Users\\goutsmedt\\Documents\\MEGAsync\\Zotero\\storage\\JK249PLU\\Griffiths et Steyvers - 2004 - Finding scientific topics.pdf}
}
